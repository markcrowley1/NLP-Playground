{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PII Data Detection\n",
    "\n",
    "First approach to the [PII Data Detection competition](https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/overview) posted on Kaggle.\n",
    "\n",
    "The goal of this competition is to develop a model that detects personally identifiable information (PII) in student writing/essays.\n",
    "\n",
    "This notebook will focus on the development of simple model trained on the competition dataset containing approximately 22,000 essays. The model should assign labels to the following seven types of PII:\n",
    "\n",
    "| Label         | Description                                                                                                                                        |\n",
    "|---------------|----------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| NAME_STUDENT  | The full or partial name of a student that is not necessarily the author of the essay. This excludes instructors, authors, and other person names. | \n",
    "| EMAIL         | A student’s email address.                                                                                                                         |\n",
    "| USERNAME      | A student's username on any platform.                                                                                                              |\n",
    "| ID_NUM        | A number or sequence of characters that could be used to identify a student, such as a student ID or a social security number.                     |\n",
    "| PHONE_NUM     | A phone number associated with a student.                                                                                                          |\n",
    "| URL_PERSONAL  | A URL that might be used to identify a student.                                                                                                    |\n",
    "| STREET_ADDRESS | A full or partial street address that is associated with the student, such as their home address.                                                  |\n",
    "\n",
    "## Metadata\n",
    "\n",
    "The competition dataset contains a compilation of original documents along with corresponding tokens that were generated using the SpaCy English tokeniser. There are corresponding labels for each token, presented in the BIO format. This means that the first token of a PII entity is labelled with a prefix 'B-', and the following tokens representing the entity are labelled with a prefix \"I-\". Non-PII tokens are labeled \"O\". There are also a few extra fields in the JSON data, which are detailed in the table below:\n",
    "\n",
    "| Field               | Description                                                                                                                                        |\n",
    "|---------------------|------------------------------------------------------------------------|\n",
    "| document            | Integer ID of the essay                                                | \n",
    "| full_text           | UTF-8 representation of the essay.                                     |\n",
    "| tokens              | String representation of each token.                                   |\n",
    "| trailing_whitespace | Boolean value indicating whether each token is followed by whitespace. |\n",
    "| labels              | Token label in BIO format.                                             |\n",
    "\n",
    "Download the competition dataset [here](https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/data?select=train.json).\n",
    "\n",
    "## Approach\n",
    "\n",
    "State of the art methods utilise large pretrained transformer based language models such as DeBERTa for named entity recognition. However, I decided to employ a much smaller LSTM model in this approach, prioritising lower memory usage. A tokenizer was trained from scratch using the byte pair encoding algorithm. This was done in the hope of capturing the value of subword tokenisation. A pretrained tokenizer was not used so that the vocab size would be kept small, and hopefully relevant to the text in the essays used for the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in training and test data\n",
    "with open(\"./PII_Data/train.json\", 'r') as f:\n",
    "    json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "753\n",
      "['Design', 'Thinking', 'for', 'innovation', 'reflexion', '-', 'Avril', '2021', '-', 'Nathalie', 'Sylla', '\\n\\n', 'Challenge', '&', 'selection', '\\n\\n', 'The', 'tool', 'I', 'use', 'to', 'help', 'all', 'stakeholders', 'finding', 'their', 'way', 'through', 'the', 'complexity', 'of', 'a', 'project', 'is', 'the', ' ', 'mind', 'map', '.', '\\n\\n', 'What', 'exactly', 'is', 'a', 'mind', 'map', '?', 'According', 'to', 'the']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NAME_STUDENT', 'I-NAME_STUDENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "# Take a look at some tokens and corresponding labels\n",
    "for i in range(1):\n",
    "    print(len(json_data[i][\"tokens\"]))\n",
    "    print(json_data[i][\"tokens\"][:50])\n",
    "    print(json_data[i][\"labels\"][:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test splits from data\n",
    "# random.shuffle(json_data) # shuffle data for randomised train/test split\n",
    "train_ratio = 0.8\n",
    "n_train_samples = int(len(json_data) * train_ratio)\n",
    "train_data = json_data[:n_train_samples]\n",
    "test_data = json_data[n_train_samples:]\n",
    "\n",
    "del json_data\n",
    "\n",
    "# Get raw text for training tokenizer\n",
    "train_docs = [sample[\"full_text\"] for sample in train_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[329, 279, 330, 257, 773, 265, 295, 511, 13, 722, 72, 13, 13, 13]\n",
      "['Th', 'is', 'Ġis', 'Ġa', 'Ġtest', 'Ġs', 'ent', 'ence', '.', 'ĠH', 'i', '.', '.', '.']\n",
      "381 ['Ġour', 'Ġle', 'Ġtest', 'Ġhas', 'Ġma', 'ĠSt', 'ĠThis', 'Ġsit', 'Ġident', 'Ġal', 'Ġrequire', 'Ġabout', 'ĠâĢ', 'Ġlearn', 'Ġem', 'Ġa', 'Ġdis', 'Ġpo', 'Ġ1', 'Ġj', 'Ġlot', 'Ġwill', 'Ġused', 'Ġstarted', 'Ġfoc', 'Ġworking', 'Ġknow', 'Ġonly', 'Ġcl', 'Ġtr', 'Ġthat', 'Ġ2', 'Ġdifferent', 'ĠW', 'Ġbeen', 'Ġch', 'Ġdo', 'ĠD', 'Ġup', 'Ġob', 'Ġmore', 'Ġat', 'Ġneed', 'Ġtechn', 'Ġwhen', 'Ġbec', 'Ġli', 'Ġcons', 'Ġwho', 'Ġbe']\n"
     ]
    }
   ],
   "source": [
    "# Train BPE tokenizer\n",
    "VOCAB_SIZE = 1000\n",
    "bpe_tokenizer = ByteLevelBPETokenizer()\n",
    "bpe_tokenizer.train_from_iterator(train_docs, vocab_size=VOCAB_SIZE, min_frequency=2)\n",
    "\n",
    "# Add special tokens\n",
    "special_tokens = [\"<PAD>\", \"<STARTOFTEXT>\", \"<ENDOFTEXT>\"]\n",
    "bpe_tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "enc = bpe_tokenizer.encode(\"This is a test sentence. Hi...\")\n",
    "vocab = bpe_tokenizer.get_vocab()\n",
    "word_boundary_tokens = [token for token in vocab if token.startswith('Ġ')]\n",
    "print(enc.ids)\n",
    "print(enc.tokens)\n",
    "print(len(word_boundary_tokens), word_boundary_tokens[:50]) # Ġ represents the beginning of a word\n",
    "\n",
    "# Pad token\n",
    "pad_enc = bpe_tokenizer.encode(\"<PAD>\").ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'B-EMAIL', 1: 'B-ID_NUM', 2: 'B-NAME_STUDENT', 3: 'B-PHONE_NUM', 4: 'B-STREET_ADDRESS', 5: 'B-URL_PERSONAL', 6: 'B-USERNAME', 7: 'I-ID_NUM', 8: 'I-NAME_STUDENT', 9: 'I-PHONE_NUM', 10: 'I-STREET_ADDRESS', 11: 'I-URL_PERSONAL', 12: 'O', 13: '<PAD>'}\n"
     ]
    }
   ],
   "source": [
    "# Define the relevant labels within the data\n",
    "targets = [\n",
    "    'B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', \n",
    "    'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM', \n",
    "    'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL'\n",
    "]\n",
    "label_set = targets + [\"O\"] + [\"<PAD>\"]\n",
    "label2id = {l: i for i,l in enumerate(label_set)}\n",
    "id2label = {v:k for k,v in label2id.items()}\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1346 1346\n",
      "[('0', 'O'), ('2', 'O'), ('1', 'O'), ('-', 'O'), ('N', 'B-NAME_STUDENT'), ('at', 'B-NAME_STUDENT'), ('h', 'B-NAME_STUDENT'), ('al', 'B-NAME_STUDENT'), ('ie', 'B-NAME_STUDENT'), ('ĠS', 'I-NAME_STUDENT'), ('y', 'I-NAME_STUDENT'), ('ll', 'I-NAME_STUDENT'), ('a', 'I-NAME_STUDENT'), ('ĊĊ', 'O'), ('Challenge', 'O'), ('Ġ&', 'O'), ('Ġse', 'O'), ('lection', 'O'), ('ĊĊ', 'O'), ('The', 'O'), ('Ġtool', 'O'), ('ĠI', 'O'), ('Ġuse', 'O'), ('Ġto', 'O'), ('Ġhelp', 'O'), ('Ġall', 'O'), ('Ġstakeholders', 'O'), ('Ġfind', 'O'), ('ing', 'O'), ('Ġtheir', 'O'), ('Ġway', 'O'), ('Ġthrough', 'O'), ('Ġthe', 'O'), ('Ġcomple', 'O'), ('x', 'O'), ('ity', 'O'), ('Ġof', 'O'), ('Ġa', 'O'), ('Ġproject', 'O'), ('Ġis', 'O'), ('Ġthe', 'O'), ('ĠĠ', 'O'), ('m', 'O'), ('ind', 'O'), ('Ġmap', 'O'), ('.', 'O'), ('ĊĊ', 'O'), ('W', 'O'), ('hat', 'O'), ('Ġex', 'O')]\n"
     ]
    }
   ],
   "source": [
    "def relabel_tokens(tokenizer, train_data: list, debug=False):\n",
    "    \"\"\" Generate tokens with newly created vocab, mapping relevant labels to new tokens. \"\"\"\n",
    "\n",
    "    prev_ws: bool = False\n",
    "    new_tokens = []\n",
    "    new_labels = []\n",
    "    idx = 0\n",
    "    token_map = []\n",
    "    for t, l, ws in zip(train_data[\"tokens\"], train_data[\"labels\"], train_data[\"trailing_whitespace\"]):\n",
    "        # Add space to this string if required\n",
    "        if prev_ws is True:\n",
    "            curr = \" \" + t\n",
    "        else:\n",
    "            curr = t\n",
    "        # Keep track of next string ws required\n",
    "        if ws is True:\n",
    "            prev_ws = True\n",
    "        else:\n",
    "            prev_ws = False\n",
    "        # Create new tokens/labels\n",
    "        enc = tokenizer.encode(curr)\n",
    "        if debug is False:\n",
    "            new_tokens += enc.ids\n",
    "            new_labels += [label2id[l]] * len(enc.tokens)\n",
    "        else:\n",
    "            new_tokens += enc.tokens\n",
    "            new_labels += [l] * len(enc.tokens)\n",
    "        # Map new tokens to original token\n",
    "        token_map.extend([idx] * len(enc.tokens))\n",
    "        idx += 1\n",
    "\n",
    "    return list(zip(new_tokens, new_labels)), token_map\n",
    "\n",
    "labelled_tokens, token_map = relabel_tokens(bpe_tokenizer, train_data[0], debug=True)\n",
    "print(len(labelled_tokens), len(token_map))\n",
    "print(labelled_tokens[20:70])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "753 [('Design', 'O'), ('Thinking', 'O'), ('for', 'O'), ('innovation', 'O'), ('reflexion', 'O'), ('-', 'O'), ('Avril', 'O'), ('2021', 'O'), ('-', 'O'), ('Nathalie', 'B-NAME_STUDENT'), ('Sylla', 'I-NAME_STUDENT'), ('\\n\\n', 'O'), ('Challenge', 'O'), ('&', 'O'), ('selection', 'O'), ('\\n\\n', 'O'), ('The', 'O'), ('tool', 'O'), ('I', 'O'), ('use', 'O')]\n"
     ]
    }
   ],
   "source": [
    "def map_predictions(labelled_tokens: list[tuple], token_map: list, original_tokens: list):\n",
    "    \"\"\" Map NER predictions back to original tokens. \"\"\"\n",
    "    prev_idx = -1\n",
    "    predictions = []\n",
    "    for item, idx in zip(labelled_tokens, token_map):\n",
    "        if idx != prev_idx:\n",
    "            token, label = item\n",
    "            predictions.append((original_tokens[idx], label))\n",
    "            prev_idx = idx\n",
    "    return predictions\n",
    "\n",
    "predictions = map_predictions(labelled_tokens, token_map, train_data[0][\"tokens\"])\n",
    "print(predictions == list(zip(train_data[0][\"tokens\"], train_data[0][\"labels\"])))\n",
    "print(len(predictions), predictions[:20])\n",
    "del predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now developed a method to break down our word sized tokens into smaller subword tokens, which removes the need to use \"unk\" tokens for previously unseen words during inference. Next, we are required to devise a method to break up our documents into smaller sequences for training. We should make sure that 'I' tags are included in the same sequence as their corresponding 'B' tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ions', ',', 'Ġwe', 'Ġcan', 'Ġuse', ':', 'Ġwho', ',', 'Ġwhat', ',', 'ĠĠ', 'w', 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000)\n"
     ]
    }
   ],
   "source": [
    "def gen_sequences(labelled_tokens, context_length, padding):\n",
    "    \"\"\" Split up token sequences into smaller sequences for training. \"\"\"\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    pad_enc = bpe_tokenizer.encode(\"<PAD>\")\n",
    "    pad_label = label2id[\"<PAD>\"]\n",
    "    for i in range(0, len(labelled_tokens), context_length):\n",
    "        label = \"\"\n",
    "        sequence = []\n",
    "        # End on 'o' label to avoid seperating B and I\n",
    "        while (len(sequence) < context_length - padding or label != 'O') and len(sequence) + i < len(labelled_tokens):\n",
    "            idx = len(sequence) + i\n",
    "            token, label = labelled_tokens[idx]\n",
    "            sequence.append(labelled_tokens[idx])\n",
    "            if len(sequence) >= context_length: # give up at this point\n",
    "                break\n",
    "\n",
    "        sequence.extend([(pad_enc.ids[0], pad_label)] * (context_length - len(sequence)))\n",
    "        tokens, labels = zip(*sequence)\n",
    "        train_x.append(tokens)\n",
    "        train_y.append(labels)\n",
    "    return train_x, train_y\n",
    "\n",
    "# Now testing this function\n",
    "CONTEXT_LENGTH = 128\n",
    "PADDING = 28\n",
    "x, y = gen_sequences(labelled_tokens, CONTEXT_LENGTH, PADDING)\n",
    "print(x[5][-40:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now generate training data\n",
    "CONTEXT_LENGTH = 128\n",
    "PADDING = 28\n",
    "train_x = []\n",
    "train_y = []\n",
    "for document in train_data:\n",
    "    labelled_tokens, token_map = relabel_tokens(bpe_tokenizer, document)\n",
    "    tokens, labels = gen_sequences(labelled_tokens, CONTEXT_LENGTH, PADDING)\n",
    "    train_x += tokens\n",
    "    train_y += labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58109 128\n",
      "58109 128\n",
      "(701, 260, 445, 990, 304, 289, 831, 533, 600, 420, 13, 661, 35, 436, 346, 258, 396, 333, 285, 879, 304, 307, 69, 288, 87, 282, 12, 32, 85, 81, 415, 799, 15, 17, 16, 12, 45, 264, 71, 281, 471, 421, 88, 308, 64, 661, 32, 77, 703, 87, 822, 25, 465, 407, 465, 708, 421, 71, 474, 67, 284, 459, 415, 651, 565, 661, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(len(train_x), len(train_x[100]))\n",
    "print(len(train_y), len(train_y[100]))\n",
    "print(train_x[10])\n",
    "\n",
    "train_x_tensor = torch.tensor(train_x)\n",
    "train_y_tensor = torch.tensor(train_y)\n",
    "dataset = TensorDataset(train_x_tensor, train_y_tensor)\n",
    "BATCH_SIZE = 32\n",
    "train_data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Recurrent Neural Network \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, label_set):\n",
    "        super(Net, self).__init__()\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, len(label_set))\n",
    "\n",
    "    def forward(self, input):\n",
    "        embeds = self.token_embeddings(input)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        fc_out = self.fc(lstm_out)\n",
    "        return F.log_softmax(fc_out, dim=1)\n",
    "    \n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        # Reshape targets and outputs\n",
    "        targets = targets.view(-1)\n",
    "        outputs = outputs.view(-1, len(label_set))\n",
    "        # Mask out padding tokens\n",
    "        mask = (targets != label2id[\"<PAD>\"]).float()\n",
    "        num_tokens = int(torch.sum(mask))\n",
    "        outputs = outputs[range(outputs.shape[0]), targets] * mask\n",
    "        # Calculate and return loss\n",
    "        return -torch.sum(outputs)/num_tokens\n",
    "    \n",
    "# Train Loop\n",
    "def train(model, optimizer, loss_function, train_data_loader, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for tokens, targets in train_data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(tokens)\n",
    "            loss = loss_function(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_data_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 4.822322313480965\n",
      "Epoch 2/10, Loss: 4.821358713547038\n",
      "Epoch 3/10, Loss: 4.821168584708075\n",
      "Epoch 4/10, Loss: 4.820999118987684\n",
      "Epoch 5/10, Loss: 4.820978017630556\n",
      "Epoch 6/10, Loss: 4.820925838621703\n",
      "Epoch 7/10, Loss: 4.820893440477649\n",
      "Epoch 8/10, Loss: 4.820859763328199\n",
      "Epoch 9/10, Loss: 4.820866241854193\n",
      "Epoch 10/10, Loss: 4.8207827630547175\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "vocab_size = bpe_tokenizer.get_vocab_size()\n",
    "embedding_dim = 100\n",
    "hidden_dim = 64\n",
    "epochs = 10\n",
    "\n",
    "# Initialise and train model\n",
    "model = Net(vocab_size, embedding_dim, hidden_dim, label_set)\n",
    "loss_fn = CustomLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "train(model, optimizer, loss_fn, train_data_loader, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test data\n",
    "test_x = []\n",
    "test_y = []\n",
    "for document in test_data:\n",
    "    labelled_tokens, token_map = relabel_tokens(bpe_tokenizer, document)\n",
    "    tokens, labels = gen_sequences(labelled_tokens, CONTEXT_LENGTH, PADDING)\n",
    "    test_x += tokens\n",
    "    test_y += labels\n",
    "\n",
    "test_x_tensor = torch.tensor(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13775\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = []\n",
    "    for inputs, labels in zip(test_x_tensor, test_y):\n",
    "        outputs = model(inputs.unsqueeze(0))\n",
    "        _, predicted_labels = torch.max(outputs ,dim=2)\n",
    "        predictions.append(predicted_labels.squeeze().tolist())\n",
    "\n",
    "print(len(predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.16165690460354581\n"
     ]
    }
   ],
   "source": [
    "# Calculate Accuracy\n",
    "# Flatten the predictions and ground truth labels, excluding padding tokens\n",
    "flat_predictions = []\n",
    "flat_labels = []\n",
    "\n",
    "for pred, true in zip(predictions, test_y):\n",
    "    for p, t in zip(pred, true):\n",
    "        if t != label2id[\"<PAD>\"]:  # Exclude padding tokens\n",
    "            flat_predictions.append(p)\n",
    "            flat_labels.append(t)\n",
    "\n",
    "# Calculate the F1-score\n",
    "f1 = f1_score(flat_labels, flat_predictions, average='micro')\n",
    "\n",
    "print(f'F1-score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have succeeded in training a model that achieved and F1 score of 0.1616 on a set of test data. We can also note that the tokenisation technique that was implemented functioned properly and there was no issue with previously unseen tokens. This score is relatively low for this task and could be improved with a more complex model architecture among other techniques, but it is fair to say that the model does have some capability of identifying personal information within academic essays."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
